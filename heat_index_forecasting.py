# -*- coding: utf-8 -*-
"""Copy of heat_index_forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NMTp7WCmSMFG0Y-Aiw5FBi38kn-P9tUD

## **1.0 Import Necessary Libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle

"""## **2.0 Data Gathering**

### 2.1 Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive/')

"""### 2.2 Read CSV Dataset from Drive

#### 2.2.1 Heat Index Dataset
"""

heat_index_data = pd.read_csv('drive/MyDrive/projects/city_heat_index.csv')

heat_index_data

"""#### 2.2.1 Weather Dataset"""

weather_data = pd.read_csv('drive/MyDrive/projects/city_weather_data.csv')

weather_data

"""## **3.0 Data Cleaning**

### 3.1 Standardize Formats
"""

weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])
weather_data['datetime'] = pd.to_datetime(weather_data['datetime'].dt.strftime('%Y-%m-%d'))

heat_index_data['date_str'] = heat_index_data['year'].astype(str) + '-' + \
                                  heat_index_data['month'].astype(str).str.zfill(2) + '-' + \
                                  heat_index_data['day'].astype(str).str.zfill(2)

heat_index_data['datetime'] = pd.to_datetime(heat_index_data['date_str'], format='%Y-%m-%d')

heat_index_data = heat_index_data.drop('date_str', axis=1)

weather_data.head()

heat_index_data.head()

"""### 3.2 Merge Datasets"""

# Merging datasets
merged_data = pd.merge(weather_data, heat_index_data, on=['city_name', 'datetime'], how='inner')

merged_data

# List of columns to drop - customize this list based on your specific needs
columns_to_drop = ['sys.type', 'sys.id', 'rain.1h', 'extraction_date_time', 'weather.icon', 'year']

# Drop the columns
merged_data = merged_data.drop(columns=columns_to_drop)

# Add columns
merged_data['day_of_week'] = merged_data['datetime'].dt.dayofweek

merged_data.columns

"""### 3.2 Handling Invalid Data"""

merged_data.info()

# Apply linear interpolation
merged_data.interpolate(method='linear', inplace=True)

merged_data.info()

# Identify rows with invalid data
invalid_data = merged_data[merged_data.isin([-999] or merged_data.isna).any(axis=1)]

# Print invalid data
print("Invalid Data Rows:")
invalid_data

# Calculate monthly average heat index
monthly_avg_heat_index = merged_data.groupby(merged_data["datetime"].dt.month)['avg_heat_index_celsius'].mean()

# Create a new column 'Month' for grouping
merged_data['Month'] = merged_data["datetime"].dt.month

# Group data by month for boxplot
monthly_data = [merged_data[merged_data['Month'] == month]['avg_heat_index_celsius'] for month in range(1, 5)]

# Visualize monthly data distribution
plt.figure(figsize=(12, 8))
plt.boxplot(monthly_data, patch_artist=True)
plt.title('Distribution of Average Heat Index °C by Month')
plt.xlabel('Month')
plt.ylabel('Average Heat Index °C')
plt.show()

# Apply linear interpolation to fill missing values (-999)
merged_data['avg_heat_index_celsius'].replace(-999, np.nan, inplace=True)
merged_data['avg_heat_index_celsius'].interpolate(method='linear', inplace=True)

"""### 3.3 Verify Cleaning"""

# Check existing invalid data
merged_data[merged_data.isin([-999] or merged_data.isna).any(axis=1)]

# Create a new column 'Month' for grouping
merged_data['Month'] = merged_data["datetime"].dt.month

# Group data by month for boxplot
monthly_data = [merged_data[merged_data['Month'] == month]['avg_heat_index_celsius'] for month in range(1, 5)]

# Visualize monthly data distribution
plt.figure(figsize=(12, 8))
plt.boxplot(monthly_data, patch_artist=True)
plt.title('Distribution of Average Heat Index °C by Month')
plt.xlabel('Month')
plt.ylabel('Average Heat Index °C')
plt.show()

"""## **4.0 Data Exploration**

#### 4.1 Data Overview
"""

# Get unique cities in the DataFrame
unique_cities = merged_data['city_name'].unique()

unique_cities

merged_data.shape

merged_data.info()

"""### 4.2 Summary Statistics"""

merged_data.describe()

"""### 4.3 Data Visualization"""

# Average Heat Index per City

# Calculate average heat index for each city
average_heat_index_per_city = merged_data.groupby('city_name')['avg_heat_index_celsius'].mean().reset_index()

# Set up the plot
plt.figure(figsize=(12, 6))

# Create bar chart
sns.barplot(data=average_heat_index_per_city, x='city_name', y='avg_heat_index_celsius')

# Add title and labels
plt.title('Average Heat Index for Each City')
plt.xlabel('City')
plt.ylabel('Average Heat Index °C')
plt.xticks(rotation=84)  # Rotate x-axis labels for better readability

# Show plot
plt.tight_layout()
plt.show()

"""#### 4.3.1 Linear Regression Scatter Plot"""

# Convert index from datetime.date to datetime and then to numeric timestamps for plotting
numeric_dates = pd.to_datetime(merged_data['datetime']).astype(int) / 10**9
plt.figure(figsize=(10, 6))
sns.scatterplot(x=numeric_dates, y=merged_data['avg_heat_index_celsius'], color='blue')

# Perform linear regression
slope, intercept, r_value, p_value, std_err = stats.linregress(numeric_dates, merged_data['avg_heat_index_celsius'])
line = slope * numeric_dates + intercept

# Plot the linear regression line
plt.plot(numeric_dates, line, color='red', label='Linear Regression')

# Set plot title, labels, and legend
plt.title('Linear Regression Scatter Plot of Daily Average Heat Index Over Time')
plt.xlabel('Date')
plt.ylabel('Daily Average Heat Index °C')
plt.grid(True)
plt.legend()

# Adjust x-axis ticks to show dates instead of numeric timestamps
plt.xticks(ticks=numeric_dates, labels=[dt.strftime('%Y-%m-%d') for dt in merged_data['datetime']])
plt.xlim(numeric_dates.min(), numeric_dates.max())

plt.show()

# Copy the DataFrame
temp_df = merged_data.copy()

# Pivot the DataFrame to have cities as rows, months as columns, and heat index as values
heatmap_data_month = temp_df.pivot_table(index='city_name', columns='month', values='avg_heat_index_celsius', aggfunc='mean')

# Plot heatmap for months
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)  # Subplot for months
sns.heatmap(heatmap_data_month, cmap='YlOrRd', linewidths=0.5)
plt.title('Heatmap of Average Heat Index for Cities Over Months')
plt.xlabel('Month')
plt.ylabel('City')

# Pivot the DataFrame to have cities as rows, days as columns, and heat index as values
heatmap_data_day = temp_df.pivot_table(index='city_name', columns='day', values='avg_heat_index_celsius', aggfunc='mean')

# Plot heatmap for days
plt.subplot(2, 1, 2)  # Subplot for days
sns.heatmap(heatmap_data_day, cmap='YlOrRd', linewidths=0.5)
plt.title('Heatmap of Average Heat Index for Cities Over Days')
plt.xlabel('Day')
plt.ylabel('City')

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

"""## **5.0 Correlation**"""

plt.figure(figsize=(16,9))
sns.heatmap(merged_data.corr(numeric_only=True), annot=True, cmap="YlGnBu")

"""## **6.0 Data Preparation and Model Training**"""

# Initialize an empty dictionary to store train-test split data
train_test_data = {}

for city in merged_data['city_name'].unique():
    city_data = merged_data[merged_data['city_name'] == city]
    features = ['main.temp', 'main.feels_like', 'main.temp_min', 'main.temp_max', 'main.pressure', 'wind.speed', 'wind.gust', 'main.humidity', 'main.sea_level', 'main.grnd_level', 'month', 'day', 'weather.id', 'coord.lat', 'coord.lon']
    city_target = city_data['avg_heat_index_celsius']  # Modified to use city_data instead of merged_data
    city_data.loc[:, features] = city_data[features].fillna(method='ffill')

    # Perform train-test split
    X_train, X_test, y_train, y_test = train_test_split(city_data[features], city_target, test_size=0.2, random_state=42)

    # Standardize the data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    y_train_scaled = y_train.copy()

    # Train the model
    model = LinearRegression()
    model.fit(X_train_scaled, y_train_scaled)
    print(f"{city} Model Score: {model.score(scaler.transform(X_test), y_test)}")

    # Store train-test split data and model in the dictionary
    train_test_data[city] = {
        'X_train': X_train_scaled,
        'X_test': X_test,
        'y_train': y_train_scaled,
        'y_test': y_test,
        'model': model,
        'scaler': scaler
    }

    filename = f'{city}_model.pkl'
    with open(filename, 'wb') as file:
      pickle.dump(model, file)

"""## **7.0 Model Evaluation**"""

for city, data in train_test_data.items():
  # Retrieve the test set and model from the dictionary
  X_test = data['X_test']
  y_test = data['y_test']
  model = data['model']
  scaler = data['scaler']

  # Scale the X_test data
  X_test_scaled = scaler.transform(X_test)

  # Predict using the model
  y_pred = model.predict(X_test_scaled)

  # Calculate metrics
  mse = mean_squared_error(y_test, y_pred)
  mae = mean_absolute_error(y_test, y_pred)
  rmse = np.sqrt(mse)
  r2 = r2_score(y_test, y_pred)

  # Print the evaluation results
  print(f"Evaluation Results for {city}:")
  print(f"  Mean Squared Error: {mse:.2f}")
  print(f"  Mean Absolute Error: {mae:.2f}")
  print(f"  Root Mean Squared Error: {rmse:.2f}")
  print(f"  R^2 Score: {r2:.2f}\n")

"""### **8.0 Model Prediction**"""

prediction_data = pd.read_csv('drive/MyDrive/projects/prediction_data.csv')

prediction_data

prediction_data['datetime'] = pd.to_datetime(prediction_data['datetime'])
prediction_data['datetime'] = pd.to_datetime(prediction_data['datetime'].dt.strftime('%Y-%m-%d'))

# Handle Missing Values
prediction_data = prediction_data.interpolate()

prediction_data['month'] = prediction_data['datetime'].dt.month
prediction_data['day'] = prediction_data['datetime'].dt.day

# Define the list of features your model was trained on
model_features = ['main.temp', 'main.feels_like', 'main.temp_min', 'main.temp_max', 'main.pressure',
                  'wind.speed', 'wind.gust', 'main.humidity', 'main.sea_level', 'main.grnd_level', 'month', 'day',
                  'weather.id', 'coord.lat', 'coord.lon', 'city_name']

# Filter the future_data DataFrame to include only the features needed for the model
filtered_data = prediction_data[model_features]

merged_data['city_name'].unique()

def process_city(city, start_date):
    scaler = train_test_data[city]['scaler']

    # Filter the DataFrame for the specified city
    predicted_city_data = filtered_data[filtered_data['city_name'] == city]
    predicted_city_data.drop(['city_name'], axis=1, inplace=True)

    # Convert month and day to datetime format and ensure year is included correctly
    predicted_city_data['datetime'] = pd.to_datetime(
        "2024" + '-' +
        predicted_city_data['month'].astype(str) + '-' +
        predicted_city_data['day'].astype(str),
        errors='coerce'
    )

    # Filter data to include only dates on or after the start_date
    predicted_city_data = predicted_city_data[predicted_city_data['datetime'] >= pd.to_datetime(start_date)]

    # Resample data daily and aggregate as required
    resampled_data = predicted_city_data.resample('D', on='datetime').agg({
        'main.temp': 'mean',
        'main.feels_like': 'mean',
        'main.temp_min': 'min',
        'main.temp_max': 'max',
        'main.pressure': 'mean',
        'wind.speed': 'mean',
        'wind.gust': 'max',
        'main.humidity': 'mean',
        'main.sea_level': 'mean',
        'main.grnd_level': 'mean',
        'month': 'first',
        'day': 'first',
        'weather.id': 'first',
        'coord.lat': 'first',
        'coord.lon': 'first',
    })

    # Scale the resampled data
    return scaler.transform(resampled_data), resampled_data

# Usage
city = 'Surigao City'
start_date = '2024-01-01'
processed_data, daily_data = process_city(city, start_date)

model = train_test_data[city]['model']

# Make predictions
predictions = model.predict(processed_data)

# Optionally add predictions to the DataFrame
daily_data['predicted_heat_index'] = predictions

print(f"{city} Prediction Results: ")
print(daily_data[['month', 'day', 'predicted_heat_index']].to_string(index=False))

plt.figure(figsize=(10, 5))  # Set the figure size as you see fit
plt.plot(daily_data['day'], daily_data['predicted_heat_index'], marker='o', linestyle='-', color='b')
plt.title(f"{city} Predicted Heat Index Over Time")
plt.xlabel('Date')
plt.ylabel('Predicted Heat Index')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()